# Go Code Generator (GCG) ğŸ› ï¸

An extensible AI Agent Orchestrator for generating Go CRUD code, Angular apps, GitHub automations, images, audio, video, and more â€” using local and remote LLMs!

Built with:

- ğŸ§  Python 3
- ğŸ–¥ï¸ Streamlit (UI)
- ğŸ“œ YAML-based workflow definitions
- ğŸŒ OpenAI / Ollama / DeepSeek integrations
- ğŸ³ Docker optional support

---

# ğŸ“† Requirements

- Python 3.8+
- Docker (optional)
- Access to LLMs (OpenAI API key or local Ollama models)

---

# ğŸ”§ Setup Instructions

### 1. Clone the Repo
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### 2. Create and Activate Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate   # macOS/Linux
# OR
.venv\Scripts\activate      # Windows
```

### 3. Install Required Packages
```bash
pip install --no-cache-dir -r requirements.txt
```

---

# ğŸ§  Supported LLM Models

| Model | Best Use | Tags |
|:------|:---------|:-----|
| `ModelOllama` | Offline coding tasks | [Local] |
| `ModelDeepSeekCoder67` | Go/Python/SQL code | [Local], [Code] |
| `ModelGpt4Turbo` | Structured workflows | [OpenAI] |
| `ModelGpt35Turbo` | Fast, cheap drafts | [OpenAI] |
| `ModelDalle3` | Image generation | [OpenAI], [Image] |
| `ModelTTS1` | Text-to-Speech | [OpenAI], [Audio] |
| `ModelWhisper` | Speech-to-Text | [OpenAI], [Audio] |

âœ… Model Registry available via UI!

---

# ğŸ¤– Supported AI Agents

### ğŸ§  Core Agents
- `ChatAgent` â€” General chat/Q&A
- `GoCRUDAgent` â€” Full Go CRUD stack
- `AngularAppAgent` â€” Angular frontend builder

### ğŸ¨ Visual & Audio Agents
- `Dalle3Agent` â€” Text-to-image
- `SegmentedImageAgent` â€” Multiple image prompts from structured script
- `AudioAgent` â€” TTS & STT
- `SegmentedAudioAgent` â€” Per-section audio files
- `SegmentedSubtitleGeneratorAgent` â€” Subtitle (SRT) from script & audio
- `SegmentedVideoAssemblerAgent` â€” Build video from segmented media

### ğŸ—ƒï¸ RAG & Utility
- `RAGDatabaseBuilderAgent`, `RAGQueryAgent`, etc.
- `SaveToFileAgent`, `RequirementsExtractorAgent`, etc.

âœ… Dynamic Agent Registry available via UI

---

# ğŸš€ Example Usage

### CLI Mode
```bash
python3 run_cli.py --workflow examples/youtube/wf_segmented_01.yaml
```

Other CLI Options:
```bash
python3 run_cli.py --prompt_list
python3 run_cli.py --prompt_test youtube
python3 run_cli.py --validate
```

### Streamlit UI
```bash
streamlit run ui.py
```
- Browse workflows
- Select models/agents
- Run & visualize outputs!

---

# ğŸ¬ Segmented Video Generation

Structured YAML scripts now support `text` + `image_prompt` per section (intro, scene1, ...). Combine them into full narrated videos:

### Key Agents:
- `SegmentedAudioAgent`: generates per-section voice
- `SegmentedImageAgent`: creates image per scene
- `SegmentedSubtitleGeneratorAgent`: builds `.srt` file
- `SegmentedVideoAssemblerAgent`: merges all into final `.mp4`

---

# ğŸŒ OpenAI Setup

```bash
export OPENAI_API_KEY=your-key     # macOS/Linux
set OPENAI_API_KEY=your-key        # Windows
```

---

# ğŸ’» Local Models (Ollama, DeepSeek)

Install Ollama: [https://ollama.com/download](https://ollama.com/download)

```bash
ollama pull llama3
ollama run llama3

To create a local TTS system using Ollama

Install Coqui TTS:
```
pip install TTS
```
Set Up the TTS Engine
For Coqui TTS, download a pre-trained model:

```
tts --list_models
tts --model_name tts_models/en/ljspeech/tacotron2-DDC --download
```

Integrate with Ollama
Use Python to connect Ollama's output to the TTS engine:
```
import ollama
from TTS.api import TTS

# Initialize Ollama
client = ollama.Client()
response = client.generate(prompt="Hello, how can I assist you today?")

# Initialize TTS
tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
tts.tts_to_file(text=response['text'], file_path="output.wav")

```

Play the Generated Audio

Use a Python library like playsound or pydub to play the output.wav file:

```
from playsound import playsound
playsound("output.wav")
```


# Optional
export DEEPSEEK_URL=http://localhost:11434
```

---

# ğŸ“‚ Project Structure

| Folder | Description |
|--------|-------------|
| `workflows/` | YAML workflow definitions |
| `agents/` | All agent classes |
| `models/` | Model integrations |
| `schemas/` | JSON schema validation |
| `utils/` | Utility functions |
| `api/` | FastAPI-based HTTP server |
| `run_cli.py` | CLI entry point |
| `ui.py` | Streamlit frontend |

---

# ğŸ›  Docker Support

```bash
docker build -t gcg-agent .
docker run -p 8000:8000 -e OPENAI_API_KEY=your-key gcg-agent
```

Or run Streamlit UI inside Docker:

```bash
docker run -p 8501:8501 -e OPENAI_API_KEY=your-key gcg-agent streamlit run ui.py
```

---

# ğŸ“„ Workflow Expression Support

| Pattern | Example | From |
|---------|---------|------|
| `${var}` | `${topic}` | from `vars:` |
| `step.result` | `step1.result` | from previous step |
| `{{ var }}` | `{{ filename }}` | Jinja |
| `{{ step.result }}` | `{{ generate_script.result }}` | Jinja |

---

# âœ… Validation

```bash
python3 run_cli.py --validate
```

Or from the Streamlit UI > Validation page.

---

# ğŸŒ FastAPI API

Run workflows via HTTP!

```bash
export GCG_API_KEY=secret-key
python3 api/main.py
```

POST `/run-workflow`:
```json
{
  "workflow_file": "examples/youtube/wf_segmented_01.yaml"
}
```

Headers:
```http
x-api-key: secret-key
```

---

# ğŸ¤ License

MIT â€” Open to contribute & extend!

---

# ğŸŒŸ Final Words

âœ… Compose powerful workflows  
âœ… Scale AI pipelines easily  
âœ… Orchestrate agents for real-world dev, content, and automation!
